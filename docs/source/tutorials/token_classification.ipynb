{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0d2e007",
   "metadata": {},
   "source": [
    "# Finding Label Errors in Token Classification Datasets \n",
    "\n",
    "This tutorial shows how you can use cleanlab to find potential label errors in text datasets used for the NLP task of token classification. In token-classification, our data consists of a bunch of sentences (aka documents) in which every token (aka word) is labeled with one of K classes, and we train models to predict the class of each token in a new sentence. Example applications include part-of-speech-tagging or entity recognition, which is the focus on this tutorial. Here, we use CONLL-2003 named entity recognition dataset which contains 20,718 sentences and 301,361 tokens, where each token is labeled with one of the following classes:\n",
    "\n",
    "- LOC (location entity)\n",
    "- PER (person entity)\n",
    "- ORG (organization entity)\n",
    "- MISC (miscellaneous other type of entity)\n",
    "- O (other type of word that does not correspond to an entity)\n",
    "\n",
    "**Overview of what we'll do in this tutorial:** \n",
    "- Identify potential token label issues using cleanlab's `token_classification.filter.find_label_issues` method. \n",
    "- Rank sentences using cleanlab's `token_classification.rank.get_label_quality_scores` method. \n",
    "- TODO: (Clean Learning) Train a more robust model by removing problematic sentences. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da020bc",
   "metadata": {},
   "source": [
    "## 1. Install the required dependencies \n",
    "\n",
    "You can use `pip` to install all packages required for this tutorial as follows: \n",
    "\n",
    "    !pip install cleanlab "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8a08e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -nc https://data.deepai.org/conll2003.zip && mkdir data \n",
    "!unzip conll2003.zip -d data/ && rm conll2003.zip \n",
    "!wget -nc 'https://cleanlab-public.s3.amazonaws.com/TokenClassification/pred_probs.npz' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439b0305",
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "# Package installation (hidden on docs website).\n",
    "# Package versions used: cleanlab==2.0.0 numpy==1.16.6 \n",
    "# ericwang/cleanlab -b token_classification for now \n",
    "\n",
    "dependencies = [\"cleanlab\"]\n",
    "\n",
    "if \"google.colab\" in str(get_ipython()):  # Check if it's running in Google Colab\n",
    "    %pip install cleanlab  # for colab\n",
    "    cmd = ' '.join([dep for dep in dependencies if dep != \"cleanlab\"])\n",
    "    %pip install $cmd\n",
    "else:\n",
    "    missing_dependencies = []\n",
    "    for dependency in dependencies:\n",
    "        try:\n",
    "            __import__(dependency)\n",
    "        except ImportError:\n",
    "            missing_dependencies.append(dependency)\n",
    "\n",
    "    if len(missing_dependencies) > 0:\n",
    "        print(\"Missing required dependencies:\")\n",
    "        print(*missing_dependencies, sep=\", \")\n",
    "        print(\"\\nPlease install them before running the rest of this notebook.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1349304",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from cleanlab.token_classification.filter import find_label_issues \n",
    "from cleanlab.token_classification.rank import get_label_quality_scores, issues_from_scores \n",
    "from cleanlab.internal.token_classification_utils import * \n",
    "from cleanlab.token_classification.summary import * "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad75b45",
   "metadata": {},
   "source": [
    "## 2. Get `pred_probs` and `labels` \n",
    "\n",
    "`pred_probs` are out-of-sample model-predicted probabilities of the CoNLL-2003 dataset (including training, development, and testing dataset), obtained via cross-validation. To detect potential labels issues, we first get `pred_probs` and `labels`, which are both in nested-list format, such that: \n",
    "\n",
    "- `pred_probs` is a list of `np.arrays`, such that `pred_probs[i]` is the model-predicted probabilities for the tokens in the i'th sentence, and has shape `(N_i, K)`, where `N_i` is the number of word-level tokens of the `i`'th sentence. Each row of the matrix corresponds to a token `t` and contains the model-predicted probabilities that `t` belongs to each possible class, for each of the K classes. The columns must be ordered such that the probabilities correspond to class 0, 1, ..., K-1. \n",
    "        \n",
    "- `labels` is a list of lists, such that `labels[i]` is a list of given token labels of the `i`'th sentence. For dataset with K classes, labels must be in 0, 1, ..., K-1. All the classes (0, 1, ..., and K-1) MUST be present in ``labels[i]`` for some ``i``. \n",
    "\n",
    "Here, indicies are a tuple `(i, j)` unless otherwise specified, which refers to the `j`'th word-level token of the `i`'th sentence. Given that each sentence has different number of tokens, we store `pred_probs` and `labels` as `.npz` files, which can be easily converted to dictionaries. Use `read_npz` to retrieve `pred_probs` and `labels` in nested-list format. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4ebdc0",
   "metadata": {},
   "source": [
    "<details><summary>Below is the code used to read the `.npz` file. </summary> \n",
    "    \n",
    "    def read_npz(filepath): \n",
    "        data = dict(np.load(filepath)) \n",
    "        data = [data[str(i)] for i in range(len(data))] \n",
    "        return data </details> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab9d59a0",
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "def read_npz(filepath): \n",
    "    data = dict(np.load(filepath)) \n",
    "    data = [data[str(i)] for i in range(len(data))] \n",
    "    return data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "519cb80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probs = read_npz('pred_probs.npz') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc3150f",
   "metadata": {},
   "source": [
    "## 3. Use cleanlab to find potential label issues \n",
    "\n",
    "Based on the given labels and out-of-sample predicted probabilities, cleanlab can quickly help us identify label issues in our dataset. Here we request that the indices of the identified label issues be sorted by cleanlab’s self-confidence score, which measures the quality of each given label via the probability assigned to it in our model’s prediction. Given that we would like to visualize the results, we first read the files. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6e2963",
   "metadata": {},
   "source": [
    "Let's look at the top 20 examples cleanlab thinks are most likely to be incorrectly labeled. We obtain the sentences from the original files to display the word-level token label issues in context. `given_words` contains the word-level tokens in the dataset such that `given_words[i]` is a list of words of the `i`'th sentence; `given_labels` contains the given labels in the dataset such that `given_labels[i]` is a list of labels of the `i`th sentence. Note that in CoNLL-2003, the `B-` and `I-` prefixes indicates whether the tokens are at the start of an entity, which are ignored in this tutorial. Therefore, we have two sets of entities: \n",
    "\n",
    "    given_entities = ['O', 'B-MISC', 'I-MISC', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC'] \n",
    "     \n",
    "and \n",
    "\n",
    "    merge_entities = ['O', 'MISC', 'PER', 'ORG', 'LOC'] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac24b72b",
   "metadata": {},
   "source": [
    "<details><summary>Below is the code used for reading the files.</summary>\n",
    "\n",
    "    filepaths = ['data/train.txt', 'data/valid.txt', 'data/test.txt'] \n",
    "    given_entities = ['O', 'B-MISC', 'I-MISC', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC']\n",
    "    merged_entities = ['O', 'MISC', 'PER', 'ORG', 'LOC'] \n",
    "    entity_map = {entity: i for i, entity in enumerate(given_entities)} \n",
    "\n",
    "    def readfile(filepath, sep=' '): \n",
    "        lines = open(filepath)\n",
    "\n",
    "        data, sentence, label = [], [], []\n",
    "        for line in lines:\n",
    "            if len(line) == 0 or line.startswith('-DOCSTART') or line[0] == '\\n':\n",
    "                if len(sentence) > 0:\n",
    "                    data.append((sentence, label))\n",
    "                    sentence, label = [], []\n",
    "                continue\n",
    "            splits = line.split(sep) \n",
    "            word = splits[0]\n",
    "            if len(word) > 0 and word[0].isalpha() and word.isupper():\n",
    "                word = word[0] + word[1:].lower()\n",
    "            sentence.append(word)\n",
    "            label.append(entity_map[splits[-1][:-1]])\n",
    "\n",
    "        if len(sentence) > 0:\n",
    "            data.append((sentence, label))\n",
    "\n",
    "        given_words = [d[0] for d in data] \n",
    "        given_labels = [d[1] for d in data] \n",
    "\n",
    "        return given_words, given_labels \n",
    "\n",
    "    given_words, given_labels = [], [] \n",
    "\n",
    "    for filepath in filepaths: \n",
    "        words, label = readfile(filepath) \n",
    "        given_words.extend(words) \n",
    "        given_labels.extend(label)\n",
    "\n",
    "    sentences = list(map(get_sentence, given_words)) \n",
    "\n",
    "    sentences, mask = filter_sentence(sentences) \n",
    "    given_words = [words for m, words in zip(mask, given_words) if m] \n",
    "    given_labels = [labels for m, labels in zip(mask, given_labels) if m] \n",
    "\n",
    "    maps = [0, 1, 1, 2, 2, 3, 3, 4, 4] \n",
    "    given_labels = [mapping(labels, maps) for labels in given_labels] </details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f632872e",
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "filepaths = ['data/train.txt', 'data/valid.txt', 'data/test.txt'] \n",
    "given_entities = ['O', 'B-MISC', 'I-MISC', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC'] \n",
    "merged_entities = ['O', 'MISC', 'PER', 'ORG', 'LOC'] \n",
    "entity_map = {entity: i for i, entity in enumerate(given_entities)} \n",
    "\n",
    "def readfile(filepath, sep=' '): \n",
    "    lines = open(filepath)\n",
    "    \n",
    "    data, sentence, label = [], [], []\n",
    "    for line in lines:\n",
    "        if len(line) == 0 or line.startswith('-DOCSTART') or line[0] == '\\n':\n",
    "            if len(sentence) > 0:\n",
    "                data.append((sentence, label))\n",
    "                sentence, label = [], []\n",
    "            continue\n",
    "        splits = line.split(sep) \n",
    "        word = splits[0]\n",
    "        if len(word) > 0 and word[0].isalpha() and word.isupper():\n",
    "            word = word[0] + word[1:].lower()\n",
    "        sentence.append(word)\n",
    "        label.append(entity_map[splits[-1][:-1]])\n",
    "\n",
    "    if len(sentence) > 0:\n",
    "        data.append((sentence, label))\n",
    "        \n",
    "    given_words = [d[0] for d in data] \n",
    "    given_labels = [d[1] for d in data] \n",
    "    \n",
    "    return given_words, given_labels \n",
    "\n",
    "given_words, given_labels = [], [] \n",
    "\n",
    "for filepath in filepaths: \n",
    "    words, label = readfile(filepath) \n",
    "    given_words.extend(words) \n",
    "    given_labels.extend(label)\n",
    "    \n",
    "sentences = list(map(get_sentence, given_words)) \n",
    "\n",
    "sentences, mask = filter_sentence(sentences) \n",
    "given_words = [words for m, words in zip(mask, given_words) if m] \n",
    "given_labels = [labels for m, labels in zip(mask, given_labels) if m] \n",
    "\n",
    "maps = [0, 1, 1, 2, 2, 3, 3, 4, 4] \n",
    "labels = [mapping(labels, maps) for labels in given_labels] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd956e6c",
   "metadata": {},
   "source": [
    "Here, `issue` is a list of tuples `(i, j)`, which corresponds to the `j`'th token of the `i`'th sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95dc7268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleanlab found 2255 potential label issues. \n",
      "The top 20 most likely label errors:\n",
      "[(2907, 0), (19392, 0), (9962, 4), (8904, 30), (19303, 0), (12918, 0), (9256, 0), (11855, 20), (18392, 4), (20426, 28), (19402, 21), (14744, 15), (19371, 0), (4645, 2), (83, 9), (10331, 3), (9430, 10), (6143, 25), (18367, 0), (12914, 3)]\n"
     ]
    }
   ],
   "source": [
    "issues = find_label_issues(labels, pred_probs, return_indices_ranked_by='self_confidence') \n",
    "top = 20 \n",
    "print('Cleanlab found %d potential label issues. ' % len(issues)) \n",
    "print('The top %d most likely label errors:' % top) \n",
    "print(str(issues[:top])) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65421a2d",
   "metadata": {},
   "source": [
    "We show the top 20 potential label issues. Given that `O` and `MISC` are hard to distinguish and can sometimes be ambiguous, they are excluded from the examples below. They can be specified in the `exclude` argument, which is a list of tuples `(i, j)` such that tokens predicted as `merged_entities[j]` but labels as `merged_entities[i]` are ignored. In the following example, we ignore mislabels between `O` and `MISC`, which are indexed `0` and `1`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e13de188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 2907, token 0: \n",
      "\u001b[31mLittle\u001b[0m change from today's weather expected.\n",
      "Given label: PER, predicted label: O\n",
      "\n",
      "Sentence 19392, token 0: \n",
      "\u001b[31mLet\u001b[0m's march together,\" Scalfaro, a northerner himself, said.\n",
      "Given label: LOC, predicted label: O\n",
      "\n",
      "Sentence 9962, token 4: \n",
      "3. Nastja Rysich (\u001b[31mgermany\u001b[0m) 3.75\n",
      "Given label: LOC, predicted label: O\n",
      "\n",
      "Sentence 8904, token 30: \n",
      "The Spla has fought Khartoum's government forces in the south since 1983 for greater autonomy or independence of the mainly Christian and animist region from the Moslem, Arabised \u001b[31mnorth\u001b[0m.\n",
      "Given label: LOC, predicted label: O\n",
      "\n",
      "Sentence 12918, token 0: \n",
      "\u001b[31mMayor\u001b[0m Antonio Gonzalez Garcia, of the opposition Revolutionary Workers' Party, said in Wednesday's letter that army troops recently raided several local farms, stole cattle and raped women.\n",
      "Given label: PER, predicted label: O\n",
      "\n",
      "Sentence 9256, token 0: \n",
      "\u001b[31mSpring\u001b[0m Chg Hrw 12pct Chg White Chg\n",
      "Given label: LOC, predicted label: O\n",
      "\n",
      "Sentence 11855, token 20: \n",
      "\" We have seen the photos but for the moment the palace has no comment,\" a spokeswoman for \u001b[31mPrince\u001b[0m Rainier told Reuters.\n",
      "Given label: PER, predicted label: O\n",
      "\n",
      "Sentence 18392, token 4: \n",
      "Danila 28.5 16\u001b[31m/\u001b[0m12 Caribs/ up W224 Mobil.\n",
      "Given label: O, predicted label: LOC\n",
      "\n",
      "Sentence 19402, token 21: \n",
      "A Reuter consensus survey sees medical equipment group Radiometer reporting largely unchanged earnings when it publishes first half 19996/97 results next \u001b[31mWednesday\u001b[0m.\n",
      "Given label: ORG, predicted label: O\n",
      "\n",
      "Sentence 83, token 9: \n",
      "Listing London Denoms (K) 1-10-100 Sale Limits \u001b[31mUs\u001b[0m/ Uk/ Jp/ Fr\n",
      "Given label: LOC, predicted label: O\n",
      "\n",
      "Sentence 10331, token 3: \n",
      "Hapoel Haifa 3 \u001b[31mMaccabi\u001b[0m Tel Aviv 1\n",
      "Given label: O, predicted label: ORG\n",
      "\n",
      "Sentence 9430, token 10: \n",
      "The revered Roman Catholic nun was admitted to the Calcutta \u001b[31mhospital\u001b[0m a week ago with high fever and severe vomiting.\n",
      "Given label: LOC, predicted label: O\n",
      "\n",
      "Sentence 6143, token 25: \n",
      "The embattled Afghan government said last week that the Kabul-Salang highway would be opened on Monday or Tuesday following talks with the Supreme Coordination Council \u001b[31malliance\u001b[0m led by Jumbish-i-Milli movement of powerful opposition warlord General Abdul Rashid Dostum.\n",
      "Given label: ORG, predicted label: O\n",
      "\n",
      "Sentence 18367, token 0: \n",
      "\u001b[31mCan\u001b[0m/ U.s. Dollar Exchange Rate: 1.3570\n",
      "Given label: LOC, predicted label: O\n",
      "\n",
      "Sentence 12049, token 0: \n",
      "\u001b[31mBorn\u001b[0m in 1937 in the central province of Anhui, Dai came to Shanghai as a student and remained in the city as a prolific author and teacher of Chinese.\n",
      "Given label: LOC, predicted label: O\n",
      "\n",
      "Sentence 16764, token 7: \n",
      "1990 - British historian Alan John Percivale \u001b[31m(\u001b[0mA.j.p.) Taylor died.\n",
      "Given label: PER, predicted label: O\n",
      "\n",
      "Sentence 20446, token 0: \n",
      "\u001b[31mPace\u001b[0m bowler Ian Harvey claimed three for 81 for Victoria.\n",
      "Given label: PER, predicted label: O\n",
      "\n",
      "Sentence 15514, token 16: \n",
      "But one must not forget that the Osce only has limited powers there,\" said \u001b[31mCotti\u001b[0m, who is also the Swiss foreign minister.\"\n",
      "Given label: O, predicted label: PER\n",
      "\n",
      "Sentence 7525, token 12: \n",
      "Specter met Crown Prince Abdullah and Minister of Defence and Aviation Prince \u001b[31mSultan\u001b[0m in Jeddah, Saudi state television and the official Saudi Press Agency reported.\n",
      "Given label: PER, predicted label: O\n",
      "\n",
      "Sentence 2288, token 0: \n",
      "\u001b[31mSporting\u001b[0m his customary bright green outfit, the U.s. champion clocked 10.03 seconds despite damp conditions to take the scalp of Canada's reigning Olympic champion Donovan Bailey, 1992 champion Linford Christie of Britain and American 1984 and 1988 champion Carl Lewis.\n",
      "Given label: ORG, predicted label: O\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_issues(issues, given_words, pred_probs=pred_probs, given_labels=labels, \n",
    "               exclude=[(0, 1), (1, 0)], class_names=merged_entities) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d04902",
   "metadata": {},
   "source": [
    "More than half of the potential label issues are identified correctly. As shown above, some examples are ambigious and require manual checking. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d213b2b2",
   "metadata": {},
   "source": [
    "## 4. Most common word-level token mislabels \n",
    "\n",
    "It may be useful to examine the most common word-level token mislabels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4a006bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token '/' is potentially mislabeled 42 times throughout the dataset\n",
      "---------------------------------------------------------------------------------------\n",
      "labeled as class `O` but predicted to actually be class `LOC` 36 times\n",
      "labeled as class `O` but predicted to actually be class `PER` 4 times\n",
      "labeled as class `O` but predicted to actually be class `ORG` 2 times\n",
      "\n",
      "Token 'Chicago' is potentially mislabeled 27 times throughout the dataset\n",
      "---------------------------------------------------------------------------------------\n",
      "labeled as class `ORG` but predicted to actually be class `LOC` 22 times\n",
      "labeled as class `LOC` but predicted to actually be class `ORG` 3 times\n",
      "labeled as class `MISC` but predicted to actually be class `ORG` 2 times\n",
      "\n",
      "Token 'U.s.' is potentially mislabeled 21 times throughout the dataset\n",
      "---------------------------------------------------------------------------------------\n",
      "labeled as class `LOC` but predicted to actually be class `ORG` 8 times\n",
      "labeled as class `ORG` but predicted to actually be class `LOC` 6 times\n",
      "labeled as class `LOC` but predicted to actually be class `O` 3 times\n",
      "labeled as class `LOC` but predicted to actually be class `MISC` 2 times\n",
      "labeled as class `MISC` but predicted to actually be class `LOC` 1 times\n",
      "labeled as class `MISC` but predicted to actually be class `ORG` 1 times\n",
      "\n",
      "Token 'Digest' is potentially mislabeled 20 times throughout the dataset\n",
      "---------------------------------------------------------------------------------------\n",
      "labeled as class `O` but predicted to actually be class `ORG` 20 times\n",
      "\n",
      "Token 'Press' is potentially mislabeled 20 times throughout the dataset\n",
      "---------------------------------------------------------------------------------------\n",
      "labeled as class `O` but predicted to actually be class `ORG` 20 times\n",
      "\n",
      "Token 'New' is potentially mislabeled 17 times throughout the dataset\n",
      "---------------------------------------------------------------------------------------\n",
      "labeled as class `ORG` but predicted to actually be class `LOC` 13 times\n",
      "labeled as class `LOC` but predicted to actually be class `ORG` 2 times\n",
      "labeled as class `O` but predicted to actually be class `ORG` 1 times\n",
      "labeled as class `MISC` but predicted to actually be class `LOC` 1 times\n",
      "\n",
      "Token 'and' is potentially mislabeled 16 times throughout the dataset\n",
      "---------------------------------------------------------------------------------------\n",
      "labeled as class `ORG` but predicted to actually be class `O` 7 times\n",
      "labeled as class `O` but predicted to actually be class `ORG` 5 times\n",
      "labeled as class `O` but predicted to actually be class `LOC` 3 times\n",
      "labeled as class `MISC` but predicted to actually be class `ORG` 1 times\n",
      "\n",
      "Token 'Philadelphia' is potentially mislabeled 15 times throughout the dataset\n",
      "---------------------------------------------------------------------------------------\n",
      "labeled as class `ORG` but predicted to actually be class `LOC` 14 times\n",
      "labeled as class `LOC` but predicted to actually be class `ORG` 1 times\n",
      "\n",
      "Token 'Usda' is potentially mislabeled 13 times throughout the dataset\n",
      "---------------------------------------------------------------------------------------\n",
      "labeled as class `ORG` but predicted to actually be class `LOC` 7 times\n",
      "labeled as class `ORG` but predicted to actually be class `PER` 5 times\n",
      "labeled as class `ORG` but predicted to actually be class `MISC` 1 times\n",
      "\n",
      "Token 'York' is potentially mislabeled 12 times throughout the dataset\n",
      "---------------------------------------------------------------------------------------\n",
      "labeled as class `ORG` but predicted to actually be class `LOC` 11 times\n",
      "labeled as class `LOC` but predicted to actually be class `ORG` 1 times\n",
      "\n"
     ]
    }
   ],
   "source": [
    "info = common_label_issues(issues, given_words, \n",
    "                           labels=labels, \n",
    "                           pred_probs=pred_probs, \n",
    "                           class_names=merged_entities, \n",
    "                           exclude=[(0, 1), (1, 0)]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c417061",
   "metadata": {},
   "source": [
    "The above printed information is also stored as a DataFrame `info`, sorted by the number of mislabels in descending order. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35ef843",
   "metadata": {},
   "source": [
    "## 5. Find issue sentences with particular word \n",
    "\n",
    "Call `search_token` to examine the token label issues of a specific token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8f4e163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 471, token 8: \n",
      "Soccer - Keane Signs Four-year Contract With Manchester \u001b[31mUnited\u001b[0m.\n",
      "Given label: LOC, predicted label: ORG\n",
      "\n",
      "Sentence 19072, token 5: \n",
      "The Humane Society of the \u001b[31mUnited\u001b[0m States estimates that between 500,000 and one million bites are delivered by dogs each year, more than half of which are suffered by children.\n",
      "Given label: LOC, predicted label: ORG\n",
      "\n",
      "Sentence 19910, token 5: \n",
      "His father Clarence Woolmer represented \u001b[31mUnited\u001b[0m Province, now renamed Uttar Pradesh, in India's Ranji Trophy national championship and captained the state during 1949.\n",
      "Given label: LOC, predicted label: ORG\n",
      "\n",
      "Sentence 15658, token 0: \n",
      "\u001b[31mUnited\u001b[0m Nations 1996-08-29\n",
      "Given label: ORG, predicted label: LOC\n",
      "\n",
      "Sentence 19879, token 1: \n",
      "1. \u001b[31mUnited\u001b[0m States Iii (Brian Shimer, Randy Jones) one\n",
      "Given label: ORG, predicted label: LOC\n",
      "\n",
      "Sentence 19104, token 0: \n",
      "\u001b[31mUnited\u001b[0m Nations 1996-12-06\n",
      "Given label: ORG, predicted label: LOC\n",
      "\n"
     ]
    }
   ],
   "source": [
    "token_issues = filter_by_token('United', issues, given_words) \n",
    "display_issues(token_issues, given_words, pred_probs=pred_probs, given_labels=labels, \n",
    "               exclude=[(0, 1), (1, 0)], class_names=merged_entities) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1759108b",
   "metadata": {},
   "source": [
    "## 6. Sentence label quality score \n",
    "\n",
    "Cleanlab can analyze every label in the dataset and provide a numerical score for each sentence. The score ranges between 0 and 1: a lower score indicates that the sentence is more likely to contain at least one error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db0b5179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 2907, token 0: \n",
      "\u001b[31mLittle\u001b[0m change from today's weather expected.\n",
      "Given label: PER, predicted label: O\n",
      "\n",
      "Sentence 19392, token 0: \n",
      "\u001b[31mLet\u001b[0m's march together,\" Scalfaro, a northerner himself, said.\n",
      "Given label: LOC, predicted label: O\n",
      "\n",
      "Sentence 9962, token 4: \n",
      "3. Nastja Rysich (\u001b[31mgermany\u001b[0m) 3.75\n",
      "Given label: LOC, predicted label: O\n",
      "\n",
      "Sentence 8904, token 30: \n",
      "The Spla has fought Khartoum's government forces in the south since 1983 for greater autonomy or independence of the mainly Christian and animist region from the Moslem, Arabised \u001b[31mnorth\u001b[0m.\n",
      "Given label: LOC, predicted label: O\n",
      "\n",
      "Sentence 12918, token 0: \n",
      "\u001b[31mMayor\u001b[0m Antonio Gonzalez Garcia, of the opposition Revolutionary Workers' Party, said in Wednesday's letter that army troops recently raided several local farms, stole cattle and raped women.\n",
      "Given label: PER, predicted label: O\n",
      "\n",
      "Sentence 9256, token 0: \n",
      "\u001b[31mSpring\u001b[0m Chg Hrw 12pct Chg White Chg\n",
      "Given label: LOC, predicted label: O\n",
      "\n",
      "Sentence 11855, token 20: \n",
      "\" We have seen the photos but for the moment the palace has no comment,\" a spokeswoman for \u001b[31mPrince\u001b[0m Rainier told Reuters.\n",
      "Given label: PER, predicted label: O\n",
      "\n",
      "Sentence 18392, token 4: \n",
      "Danila 28.5 16\u001b[31m/\u001b[0m12 Caribs/ up W224 Mobil.\n",
      "Given label: O, predicted label: LOC\n",
      "\n",
      "Sentence 19402, token 21: \n",
      "A Reuter consensus survey sees medical equipment group Radiometer reporting largely unchanged earnings when it publishes first half 19996/97 results next \u001b[31mWednesday\u001b[0m.\n",
      "Given label: ORG, predicted label: O\n",
      "\n",
      "Sentence 83, token 9: \n",
      "Listing London Denoms (K) 1-10-100 Sale Limits \u001b[31mUs\u001b[0m/ Uk/ Jp/ Fr\n",
      "Given label: LOC, predicted label: O\n",
      "\n",
      "Sentence 10331, token 3: \n",
      "Hapoel Haifa 3 \u001b[31mMaccabi\u001b[0m Tel Aviv 1\n",
      "Given label: O, predicted label: ORG\n",
      "\n",
      "Sentence 9430, token 10: \n",
      "The revered Roman Catholic nun was admitted to the Calcutta \u001b[31mhospital\u001b[0m a week ago with high fever and severe vomiting.\n",
      "Given label: LOC, predicted label: O\n",
      "\n",
      "Sentence 6143, token 25: \n",
      "The embattled Afghan government said last week that the Kabul-Salang highway would be opened on Monday or Tuesday following talks with the Supreme Coordination Council \u001b[31malliance\u001b[0m led by Jumbish-i-Milli movement of powerful opposition warlord General Abdul Rashid Dostum.\n",
      "Given label: ORG, predicted label: O\n",
      "\n",
      "Sentence 18367, token 0: \n",
      "\u001b[31mCan\u001b[0m/ U.s. Dollar Exchange Rate: 1.3570\n",
      "Given label: LOC, predicted label: O\n",
      "\n",
      "Sentence 12049, token 0: \n",
      "\u001b[31mBorn\u001b[0m in 1937 in the central province of Anhui, Dai came to Shanghai as a student and remained in the city as a prolific author and teacher of Chinese.\n",
      "Given label: LOC, predicted label: O\n",
      "\n",
      "Sentence 16764, token 7: \n",
      "1990 - British historian Alan John Percivale \u001b[31m(\u001b[0mA.j.p.) Taylor died.\n",
      "Given label: PER, predicted label: O\n",
      "\n",
      "Sentence 20446, token 0: \n",
      "\u001b[31mPace\u001b[0m bowler Ian Harvey claimed three for 81 for Victoria.\n",
      "Given label: PER, predicted label: O\n",
      "\n",
      "Sentence 15514, token 16: \n",
      "But one must not forget that the Osce only has limited powers there,\" said \u001b[31mCotti\u001b[0m, who is also the Swiss foreign minister.\"\n",
      "Given label: O, predicted label: PER\n",
      "\n",
      "Sentence 7525, token 12: \n",
      "Specter met Crown Prince Abdullah and Minister of Defence and Aviation Prince \u001b[31mSultan\u001b[0m in Jeddah, Saudi state television and the official Saudi Press Agency reported.\n",
      "Given label: PER, predicted label: O\n",
      "\n",
      "Sentence 2288, token 0: \n",
      "\u001b[31mSporting\u001b[0m his customary bright green outfit, the U.s. champion clocked 10.03 seconds despite damp conditions to take the scalp of Canada's reigning Olympic champion Donovan Bailey, 1992 champion Linford Christie of Britain and American 1984 and 1988 champion Carl Lewis.\n",
      "Given label: ORG, predicted label: O\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scores, token_scores = get_label_quality_scores(labels, pred_probs) \n",
    "issues = issues_from_scores(scores, token_scores=token_scores) \n",
    "display_issues(issues, given_words, pred_probs=pred_probs, given_labels=labels, \n",
    "               exclude=[(0, 1), (1, 0)], class_names=merged_entities) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a18795eb",
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "highlighted_indices = [(2907, 0), (19392, 0), (9962, 4), (8904, 30), (19303, 0), \n",
    "                       (12918, 0), (9256, 0), (11855, 20), (18392, 4), (20426, 28), \n",
    "                       (19402, 21), (14744, 15), (19371, 0), (4645, 2), (83, 9), \n",
    "                       (10331, 3), (9430, 10), (6143, 25), (18367, 0), (12914, 3)] \n",
    "\n",
    "if not all(x in issues for x in highlighted_indices):\n",
    "    raise Exception(\"Some highlighted examples are missing from ranked_label_issues.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
